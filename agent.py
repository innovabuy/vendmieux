"""
VendMieux ‚Äî Agent vocal temps r√©el
Simulateur de prospect IA pour formation commerciale

Architecture :
  Micro du commercial ‚Üí Deepgram STT ‚Üí Claude LLM (persona prospect) ‚Üí ElevenLabs TTS ‚Üí Haut-parleur

L'agent joue le r√¥le d'un prospect fran√ßais r√©aliste,
g√©n√©r√© dynamiquement √† partir d'un sc√©nario FORCE 3D.
"""

import asyncio
import json
import os
import random
import time
import logging
import threading
from collections.abc import AsyncIterable
from pathlib import Path
from dotenv import load_dotenv
import httpx

from livekit import agents, rtc
from livekit.agents import (
    Agent,
    AgentSession,
    AgentServer,
    JobContext,
    JobProcess,
    RunContext,
    function_tool,
)
from livekit.agents.voice.agent import ModelSettings
from livekit.agents import llm
from livekit.plugins import deepgram, silero, anthropic
from livekit.plugins import google as google_tts
import anthropic as anthropic_sdk

from tts_utils import normalize_tts_stream


load_dotenv()
logger = logging.getLogger("vendmieux")
logger.setLevel(logging.INFO)

# --- Client Anthropic pour d√©tection de genre ---
_anthropic_client = anthropic_sdk.Anthropic()

# --- Voix TTS par genre (Google Cloud TTS ‚Äî Chirp3-HD) ---
VOIX_FEMININES = [
    'fr-FR-Chirp3-HD-Kore',          # Naturelle, claire
    'fr-FR-Chirp3-HD-Aoede',         # Alternative
    'fr-FR-Chirp3-HD-Leda',          # Alternative 2
]
VOIX_MASCULINES = [
    'fr-FR-Chirp3-HD-Charon',        # Naturel, pos√©
    'fr-FR-Chirp3-HD-Orus',          # Alternative
    'fr-FR-Chirp3-HD-Puck',          # Alternative 2
]

# --- Cache genre par scenario_id (√©vite appels Claude r√©p√©t√©s) ---
_gender_cache: dict[str, str] = {}


# Pr√©noms fran√ßais avec genre connu (√©vite un appel LLM inutile)
_PRENOMS_M = {
    "laurent", "marc", "jean", "pierre", "thomas", "michel", "philippe", "st√©phane",
    "stephane", "fr√©d√©ric", "frederic", "olivier", "nicolas", "christophe", "david",
    "patrick", "alain", "√©ric", "eric", "thierry", "bernard", "fran√ßois", "francois",
    "yves", "jacques", "gilles", "r√©mi", "remi", "mathieu", "julien", "antoine",
    "bruno", "vincent", "s√©bastien", "sebastien", "mehdi", "karim", "yannick",
    "guillaume", "fabrice", "j√©r√¥me", "jerome", "pascal", "herv√©", "herve",
    "arnaud", "didier", "serge", "denis", "emmanuel", "rapha√´l", "raphael",
    "maxime", "benjamin", "alexandre", "paul", "louis", "hugo", "lucas", "l√©o",
    "arthur", "adam", "gabriel", "nathan", "th√©o", "ethan", "noah",
    "franck", "matthieu", "bertrand", "sylvain", "√©tienne", "rachid", "g√©rard",
    "gerard", "jean-marc", "jean-pierre", "jean-luc", "jean-paul", "jean-fran√ßois",
}
_PRENOMS_F = {
    "marie", "nathalie", "isabelle", "sophie", "catherine", "sandrine", "val√©rie",
    "valerie", "christine", "c√©line", "celine", "amandine", "aur√©lie", "aurelie",
    "caroline", "anne", "claire", "julie", "laura", "√©milie", "emilie", "marine",
    "elodie", "√©lodie", "virginie", "delphine", "patricia", "sylvie", "martine",
    "fran√ßoise", "francoise", "monique", "nicole", "florence", "b√©atrice", "beatrice",
    "agathe", "l√©a", "manon", "chlo√©", "chloe", "emma", "jade", "alice", "lina",
    "sarah", "fatima", "a√Øcha", "aicha", "m√©lanie", "melanie",
    "v√©ronique", "veronique", "corinne", "mathilde", "c√©cile", "cecile",
}


def detect_gender_from_persona(prompt_persona: str) -> str:
    """
    D√©tecte le genre du persona : d'abord par lookup pr√©nom,
    puis fallback Claude Haiku si pr√©nom inconnu.
    """
    # Tenter extraction du pr√©nom depuis "Tu es {Pr√©nom}" ou "INTERLOCUTEUR 1 : {Pr√©nom}"
    import re
    m = re.search(r'(?:Tu es|INTERLOCUTEUR 1 : )([\w-]+)', prompt_persona)
    if m:
        prenom = m.group(1).lower()
        if prenom in _PRENOMS_M:
            return 'M'
        if prenom in _PRENOMS_F:
            return 'F'

    # Fallback LLM pour pr√©noms ambigus (Dominique, Camille, Claude...)
    try:
        response = _anthropic_client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=10,
            messages=[{
                "role": "user",
                "content": f"""Ce persona est-il masculin ou f√©minin ?
R√©ponds UNIQUEMENT par 'M' ou 'F'.
Persona : {prompt_persona[:300]}"""
            }]
        )
        gender = response.content[0].text.strip().upper()
        return 'F' if gender == 'F' else 'M'
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è D√©tection genre √©chou√©e, fallback M: {e}")
        return 'M'


def get_cached_gender(scenario_id: str, prompt_persona: str) -> str:
    """Retourne le genre d√©tect√©, avec cache par scenario_id."""
    if scenario_id not in _gender_cache:
        _gender_cache[scenario_id] = detect_gender_from_persona(prompt_persona)
        logger.info(f"üîç Genre d√©tect√© pour {scenario_id}: {_gender_cache[scenario_id]}")
    return _gender_cache[scenario_id]

# --- R√©pertoire des sc√©narios ---
SCENARIOS_DIR = Path(__file__).parent / "scenarios"


# --- Cache sc√©narios en m√©moire (√©vite re-conversion √† chaque session) ---
_scenarios_cache: dict[str, dict] = {}
_scenarios_db_loaded = False

# Historique conversation par room pour r√©silience reconnexion
_room_history: dict[str, dict] = {}
_room_history_lock = threading.Lock()
_ROOM_HISTORY_TTL = 900  # 15 min


def _save_room_history(room_name: str, session: AgentSession):
    """Sauvegarde l'historique de conversation pour reconnexion."""
    with _room_history_lock:
        _room_history[room_name] = {
            "history": session.history.to_dict(),
            "ts": time.time(),
        }
        # Purger les entr√©es p√©rim√©es
        cutoff = time.time() - _ROOM_HISTORY_TTL
        stale = [k for k, v in _room_history.items() if v["ts"] < cutoff]
        for k in stale:
            del _room_history[k]


def _pop_room_history(room_name: str) -> dict | None:
    """R√©cup√®re et supprime l'historique stock√©. None si absent/expir√©."""
    with _room_history_lock:
        entry = _room_history.pop(room_name, None)
        if entry and (time.time() - entry["ts"]) < _ROOM_HISTORY_TTL:
            return entry["history"]
        return None


def _ensure_scenarios_db():
    """Charge la base de sc√©narios en cache une seule fois."""
    global _scenarios_db_loaded
    if not _scenarios_db_loaded:
        from scenarios_database import load_scenarios_database
        _scenarios_cache.update(load_scenarios_database())
        _scenarios_db_loaded = True
        logger.info(f"üì¶ Cache sc√©narios initialis√© : {len(_scenarios_cache)} sc√©narios")


def _load_scenario_from_sqlite(scenario_id: str) -> dict | None:
    """Charge un sc√©nario depuis SQLite et reconstitue le format agent."""
    import sqlite3 as _sqlite3
    db_path = Path(__file__).parent / "vendmieux.db"
    if not db_path.exists():
        return None
    try:
        conn = _sqlite3.connect(str(db_path))
        conn.row_factory = _sqlite3.Row
        row = conn.execute("SELECT * FROM scenarios WHERE id = ?", (scenario_id,)).fetchone()
        conn.close()
        if not row:
            return None
        persona = json.loads(row["persona_json"]) if row["persona_json"] else {}
        objections = json.loads(row["objections_json"]) if row["objections_json"] else {}
        brief = json.loads(row["brief_json"]) if row["brief_json"] else {}
        vendeur = json.loads(row["extraction_json"]) if row["extraction_json"] else {}
        return {
            "persona": persona,
            "objections": objections,
            "brief_commercial": brief,
            "vendeur": vendeur,
            "simulation": {
                "type": row["type_simulation"] or "prospection_telephonique",
                "difficulte": row["difficulty_default"] or 2,
            },
        }
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è SQLite load failed for {scenario_id}: {e}")
        return None


def load_scenario(scenario_id: str) -> dict | None:
    """Charge un sc√©nario depuis le cache, SQLite ou les fichiers JSON."""
    _ensure_scenarios_db()
    if scenario_id in _scenarios_cache:
        return _scenarios_cache[scenario_id]
    # Multi-interlocuteurs (sc_multi_*) : les fichiers JSON contiennent persona_2/dynamique_multi
    # qui ne sont pas dans le sch√©ma SQLite ‚Üí prioriser le fichier JSON
    if scenario_id.startswith("sc_multi_"):
        filepath = SCENARIOS_DIR / f"{scenario_id}.json"
        if filepath.exists():
            with open(filepath, "r", encoding="utf-8") as f:
                scenario = json.load(f)
            _scenarios_cache[scenario_id] = scenario
            return scenario
    # SQLite (sc√©narios diversifi√©s avec persona_json mis √† jour)
    scenario = _load_scenario_from_sqlite(scenario_id)
    if scenario:
        _scenarios_cache[scenario_id] = scenario
        return scenario
    # Fallback fichiers JSON (sc√©narios g√©n√©r√©s par l'utilisateur, d√©mo, etc.)
    filepath = SCENARIOS_DIR / f"{scenario_id}.json"
    if filepath.exists():
        with open(filepath, "r", encoding="utf-8") as f:
            scenario = json.load(f)
        _scenarios_cache[scenario_id] = scenario
        return scenario
    return None


MAX_CONTEXT_ITEMS = 40  # ~20 √©changes (user + assistant) ‚Äî au-del√†, troncature
TTS_SILENCE_PREFIX_MS = 150  # Silence avant chaque r√©ponse TTS pour √©viter le premier mot coup√©
TTS_SAMPLE_RATE = 24000


def _make_silence_frame(duration_ms: int = TTS_SILENCE_PREFIX_MS,
                        sample_rate: int = TTS_SAMPLE_RATE) -> rtc.AudioFrame:
    """Cr√©e un AudioFrame de silence (int16 zeros)."""
    num_samples = int(sample_rate * duration_ms / 1000)
    silence_data = bytes(num_samples * 2)  # 2 bytes per int16 sample, all zeros
    return rtc.AudioFrame(
        data=silence_data,
        sample_rate=sample_rate,
        num_channels=1,
        samples_per_channel=num_samples,
    )


class VendMieuxProspect(Agent):
    """Agent qui joue le r√¥le d'un prospect pour VendMieux"""

    def __init__(self, system_prompt: str, scenario_name: str = ""):
        super().__init__(
            instructions=system_prompt,
        )
        self.scenario_name = scenario_name
        logger.info(f"üé≠ Prospect VendMieux initialis√© : {scenario_name}")

    def llm_node(
        self,
        chat_ctx: llm.ChatContext,
        tools: list[llm.Tool],
        model_settings: ModelSettings,
    ):
        """Tronque le contexte avant envoi au LLM pour √©viter la latence sur longues sessions."""
        chat_ctx.truncate(max_items=MAX_CONTEXT_ITEMS)
        return super().llm_node(chat_ctx, tools, model_settings)

    async def tts_node(
        self,
        text: AsyncIterable[str],
        model_settings: ModelSettings,
    ) -> AsyncIterable[rtc.AudioFrame]:
        """Override : silence de 300ms + normalisation avant TTS Google."""
        normalized_text = normalize_tts_stream(text)
        first = True
        async for frame in super().tts_node(normalized_text, model_settings):
            if first:
                yield _make_silence_frame()
                first = False
            yield frame


# --- Sc√©nario par d√©faut (hardcod√© pour le premier test) ---
DEFAULT_SCENARIO = {
    "extraction": {
        "contexte": {"type_vente": "prospection_telephonique"},
        "objectif_formation": "Passer le barrage et cr√©er l'urgence",
    },
    "persona": {
        "identite": {
            "prenom": "Olivier",
            "nom": "Bertrand",
            "age": 52,
            "poste": "Directeur G√©n√©ral",
            "entreprise": {
                "nom": "M√©capress Rh√¥ne-Alpes",
                "secteur": "Industrie m√©canique",
                "taille": "85 salari√©s",
                "ca_approximatif": "12M‚Ç¨",
            },
        },
        "psychologie": {
            "traits_dominants": ["pragmatique", "direct", "m√©fiant envers les commerciaux"],
            "motivations_profondes": [
                "R√©duire les co√ªts de maintenance",
                "Moderniser sans prendre de risque",
            ],
            "peurs_freins": [
                "Perdre du temps avec un vendeur",
                "S'engager sur une techno non √©prouv√©e",
            ],
            "rapport_aux_commerciaux": "Les tol√®re s'ils sont concrets et rapides, d√©teste le blabla commercial",
            "style_communication": "directif",
        },
        "comportement_en_rdv": {
            "ton_initial": "Neutre-froid, pas hostile mais pas accueillant",
            "signaux_interet": [
                "pose des questions techniques",
                "demande des r√©f√©rences dans son secteur",
                "√©voque ses propres probl√®mes de maintenance",
            ],
            "signaux_rejet": [
                "soupire",
                "r√©pond par monosyllabes",
                "dit 'bon √©coutez...'",
            ],
            "tics_langage": [
                "Bon...",
                "Concr√®tement ?",
                "Et donc ?",
                "√âcoutez...",
            ],
            "debit_parole": "rapide",
            "tolerance_monologue_vendeur": "15 secondes",
        },
        "contexte_actuel": {
            "situation_entreprise": "PME industrielle en croissance, 2 pannes machines impr√©vues le mois dernier qui ont co√ªt√© 40K‚Ç¨, mais le prestataire maintenance actuel est l√† depuis 15 ans",
            "priorites_actuelles": [
                "Livrer la commande Airbus √† temps",
                "Recruter 3 usineurs",
            ],
            "experience_avec_offre_similaire": "Un commercial IoT est pass√© il y a 6 mois, n'a pas convaincu",
            "fournisseur_actuel": "Maintenance Plus SARL, prestataire historique depuis 15 ans",
            "budget_disponible": "Pas de budget pr√©vu, mais pourrait d√©bloquer si ROI d√©montr√©",
        },
    },
    "objections": {
        "objections": [
            {
                "moment": "accroche",
                "verbatim": "C'est √† quel sujet ? J'ai pas beaucoup de temps l√†.",
                "type": "reflexe",
                "difficulte": 3,
                "sous_texte": "Filtrage automatique",
            },
            {
                "moment": "accroche",
                "verbatim": "On a d√©j√† un prestataire qui nous conna√Æt depuis 15 ans et qui fait le job.",
                "type": "sincere",
                "difficulte": 5,
                "sous_texte": "Relation de confiance √©tablie, peur du changement",
            },
            {
                "moment": "decouverte",
                "verbatim": "Ces nouvelles technos, c'est jamais aussi bien qu'annonc√©. Le dernier qui est venu m'a fait perdre une heure.",
                "type": "sincere",
                "difficulte": 4,
                "sous_texte": "√âchaud√© par une mauvaise exp√©rience r√©cente",
            },
            {
                "moment": "argumentation",
                "verbatim": "Rappelez-moi dans 6 mois, l√† c'est pas le moment.",
                "type": "tactique",
                "difficulte": 3,
                "sous_texte": "Veut se d√©barrasser poliment",
            },
            {
                "moment": "argumentation",
                "verbatim": "Combien √ßa co√ªte tout compris ? Parce que l√† on a z√©ro budget pour √ßa.",
                "type": "test",
                "difficulte": 4,
                "sous_texte": "Teste si le vendeur justifie ou questionne",
            },
            {
                "moment": "closing",
                "verbatim": "Envoyez-moi un mail avec une doc, je regarderai quand j'aurai 5 minutes.",
                "type": "tactique",
                "difficulte": 4,
                "sous_texte": "Fa√ßon polie de dire non sans dire non",
            },
        ],
        "objection_finale": {
            "verbatim": "Bon √©coutez, j'ai une r√©union qui commence. Si c'est vraiment int√©ressant, envoyez-moi un truc concret par mail et on verra.",
            "condition_declenchement": "Si le commercial n'a pas cr√©√© d'enjeu apr√®s 5 minutes",
        },
        "pattern_escalade": "Filtrage ‚Üí prestataire actuel ‚Üí scepticisme techno ‚Üí report ‚Üí prix ‚Üí mail poubelle",
    },
    "vendeur": {
        "entreprise": {
            "nom": "TechMaint Solutions",
            "secteur": "Maintenance pr√©dictive industrielle",
            "description": "√âditeur de solutions IoT de surveillance machines en temps r√©el",
        },
        "offre": {
            "nom": "PredictLine",
            "description": "Capteurs IoT + plateforme IA qui d√©tecte les pannes machines 48h avant qu'elles arrivent",
            "proposition_valeur": "R√©duction de 70% des arr√™ts machines non planifi√©s, ROI en 6 mois",
            "prix": "√Ä partir de 800‚Ç¨/mois pour un parc de 10 machines",
            "references": [
                "Fonderies du Rh√¥ne (72 sal.) ‚Äî 3 pannes √©vit√©es en 6 mois",
                "Pr√©cis'Usinage Lyon ‚Äî ROI atteint en 4 mois",
            ],
            "avantages_vs_concurrence": "Seule solution avec IA pr√©dictive certifi√©e pour l'usinage CN, installation en 1 journ√©e sans arr√™t de production",
        },
        "objectif_appel": {
            "type": "rdv_physique",
            "description": "D√©crocher un RDV de 30 minutes sur site pour une d√©monstration live sur une machine",
            "criteres_succes": [
                "Obtenir une date et un cr√©neau",
                "Identifier si le DG est le d√©cideur final",
                "Comprendre le parc machines et les probl√®mes r√©cents",
            ],
        },
        "contexte_appel": {
            "type": "appel_froid",
            "historique": "Premier contact, aucun historique",
            "infos_connues": "PME industrielle Rh√¥ne-Alpes, 85 salari√©s, secteur m√©canique de pr√©cision. Trouv√© via annuaire industriel.",
        },
    },
    "brief_commercial": {
        "titre": "Prospection t√©l√©phonique ‚Äî Maintenance pr√©dictive IoT",
        "vous_etes": "Commercial chez TechMaint Solutions, √©diteur de solutions IoT de maintenance pr√©dictive.",
        "vous_vendez": "PredictLine : capteurs + IA qui d√©tectent les pannes machines 48h √† l'avance. √Ä partir de 800‚Ç¨/mois.",
        "vous_appelez": "Olivier Bertrand, DG de M√©caPress Rh√¥ne-Alpes (85 sal., m√©canique de pr√©cision). Pragmatique, direct, fid√®le √† son prestataire actuel depuis 15 ans.",
        "ce_que_vous_savez": [
            "PME industrielle en croissance, secteur m√©canique de pr√©cision",
            "Trouv√© via annuaire industriel ‚Äî premier contact",
            "Le secteur souffre d'arr√™ts machines impr√©vus co√ªteux",
        ],
        "votre_objectif": "D√©crocher un RDV de 30 min sur site pour une d√©mo live.",
        "vos_atouts": [
            "R√©f√©rence : Fonderies du Rh√¥ne, 3 pannes √©vit√©es en 6 mois",
            "ROI moyen clients : 4-6 mois",
            "Installation en 1 journ√©e, sans arr√™t de production",
        ],
        "duree_estimee": "5-8 minutes",
        "niveau_difficulte": "Interm√©diaire",
    },
}


LANGUAGE_INFO = {
    "fr": {"name": "fran√ßais", "nationality": "fran√ßais(e)", "country": "la France"},
    "en": {"name": "English", "nationality": "British", "country": "the UK"},
    "es": {"name": "espa√±ol", "nationality": "espa√±ol(a)", "country": "Espa√±a"},
    "de": {"name": "Deutsch", "nationality": "deutsch", "country": "Deutschland"},
    "it": {"name": "italiano", "nationality": "italiano/a", "country": "l'Italia"},
}

# --- Greetings pr√©-script√©s (bypass LLM pour la 1√®re r√©plique) ---
GREETINGS = {
    "fr": [
        "Oui all√¥ ?",
        "All√¥, oui ?",
        "{prenom} {nom}, j'√©coute.",
        "Oui bonjour ?",
        "{entreprise}, bonjour.",
    ],
    "en": [
        "Hello?",
        "Yes, hello?",
        "{prenom} {nom} speaking.",
        "Hello, who's calling?",
        "Yes? {nom}.",
    ],
    "es": [
        "¬øS√≠, d√≠game?",
        "¬øHola?",
        "{prenom} {nom}, d√≠game.",
        "D√≠game.",
    ],
    "de": [
        "Ja, hallo?",
        "{nom}, guten Tag.",
        "Hallo, wer spricht?",
        "{nom}.",
    ],
    "it": [
        "Pronto?",
        "S√¨, pronto?",
        "{prenom} {nom}, mi dica.",
        "Pronto, {nom}.",
    ],
}


GREETINGS_MULTI = {
    "fr": [
        "[{prenom1}] Bonjour, entrez. Asseyez-vous. Je vous pr√©sente {prenom2} {nom2}, {poste2}.",
        "[{prenom1}] Bonjour. Installez-vous. Vous connaissez {prenom2} {nom2}, notre {poste2} ?",
        "[{prenom1}] Bienvenue. On est tous les deux l√†, {prenom2} et moi. On vous √©coute.",
    ],
    "en": [
        "[{prenom1}] Hello, come in. Let me introduce {prenom2} {nom2}, our {poste2}.",
        "[{prenom1}] Hi there. Have a seat. You know {prenom2} {nom2}?",
    ],
    "es": [
        "[{prenom1}] Buenos d√≠as, pasen. Les presento a {prenom2} {nom2}, {poste2}.",
    ],
    "de": [
        "[{prenom1}] Guten Tag, kommen Sie rein. Das ist {prenom2} {nom2}, unser {poste2}.",
    ],
    "it": [
        "[{prenom1}] Buongiorno, si accomodi. Le presento {prenom2} {nom2}, {poste2}.",
    ],
}


def get_greeting(language: str, persona: dict, persona_2: dict = None) -> str:
    """G√©n√®re une phrase d'accueil pr√©-script√©e (bypass LLM, gain ~2-3s)."""
    if persona_2:
        templates = GREETINGS_MULTI.get(language, GREETINGS_MULTI["fr"])
        template = random.choice(templates)
        id1 = persona.get("identite", {})
        id2 = persona_2.get("identite", {})
        return template.format(
            prenom1=id1.get("prenom", ""),
            nom1=id1.get("nom", ""),
            prenom2=id2.get("prenom", ""),
            nom2=id2.get("nom", ""),
            poste2=id2.get("poste", ""),
        )
    templates = GREETINGS.get(language, GREETINGS["fr"])
    template = random.choice(templates)
    identite = persona.get("identite", {})
    return template.format(
        prenom=identite.get("prenom", ""),
        nom=identite.get("nom", ""),
        entreprise=identite.get("entreprise", {}).get("nom", ""),
    )


PROMPT_CONSTRAINTS = """CONTRAINTE ABSOLUE PRIORIT√â MAXIMALE :
Tu ne peux JAMAIS affirmer avoir entendu quelque chose que le commercial n'a pas dit dans CETTE conversation.
Tu ne mentionnes JAMAIS de tarifs, chiffres, documents ou √©changes ant√©rieurs que le commercial n'a pas cit√©s explicitement.
Si contest√© sur des paroles : "Ah, peut-√™tre que j'ai mal compris" ‚Äî jamais "Non vous l'avez dit."
Tu ne connais PAS les r√©f√©rences clients du commercial ‚Äî tu poses des questions dessus, tu ne compl√®tes jamais.
Tu ne r√©p√®tes JAMAIS la m√™me objection plus de 2 fois. Apr√®s 2 fois : nuancer, poser une question, r√©v√©ler ta douleur cach√©e, ou mettre fin √† l'appel.
Tu ne restes JAMAIS bloqu√© en boucle sur la m√™me r√©ponse.
Tes r√©ponses sont courtes ‚Äî maximum 2-3 phrases. Tu es un prospect r√©el, pas un assistant.

R√àGLE SUR LES R√âF√âRENCES CLIENTS :
Tu peux demander UNE FOIS si le commercial a des r√©f√©rences dans ton secteur.
Si le commercial r√©pond vaguement ‚Üí tu passes √† une autre question. Tu ne bloques JAMAIS sur ce point plus d'un √©change.
Si le commercial cite une r√©f√©rence ‚Üí tu peux demander UN seul d√©tail concret (r√©sultat, d√©lai, taille d'entreprise).
Apr√®s cet √©change ‚Üí tu passes obligatoirement √† un autre sujet (prix, d√©lai, mise en place, fonctionnement).
La question des r√©f√©rences ne doit JAMAIS d√©passer 2 √©changes dans la conversation.

PREMI√àRE R√âPONSE ‚Äî INTERDIT :
Tu ne dis JAMAIS "Que voulez-vous ?", "Que puis-je pour vous ?", "Quel est l'objet de votre appel ?", "En quoi puis-je vous aider ?" comme premi√®re r√©ponse.
Au t√©l√©phone : tu d√©croches avec "All√¥ ?" ou "[Ton pr√©nom] [Ton nom], bonjour." puis tu laisses le commercial se pr√©senter.
En RDV physique : tu dis "Bonjour, asseyez-vous." ou "Bonjour, installez-vous." puis tu laisses le commercial ouvrir la conversation.
Tu ne prends JAMAIS l'initiative de demander l'objet de la visite ou de l'appel. Tu attends que le commercial s'explique.

"""


def _build_multi_prompt(scenario: dict, difficulty: int = 2, language: str = "fr") -> str:
    """Construit le prompt pour un sc√©nario multi-interlocuteurs (2 personas)."""
    p1 = scenario["persona"]
    p2 = scenario["persona_2"]
    objections = scenario["objections"]
    vendeur = scenario.get("vendeur", {})
    dynamique = scenario.get("dynamique_multi", "")

    id1 = p1["identite"]
    id2 = p2["identite"]
    name1 = f"{id1['prenom']} {id1['nom']}"
    name2 = f"{id2['prenom']} {id2['nom']}"

    traits1 = ", ".join(p1["psychologie"]["traits_dominants"])
    traits2 = ", ".join(p2["psychologie"]["traits_dominants"])

    tics1 = ", ".join(p1["comportement_en_rdv"].get("tics_langage", [])[:3]) or '"Bon...", "Concr√®tement ?"'
    tics2 = ", ".join(p2["comportement_en_rdv"].get("tics_langage", [])[:3]) or '"√âcoutez...", "On verra..."'

    ctx1 = p1["contexte_actuel"]
    ctx2 = p2.get("contexte_actuel", ctx1)

    obj_list = objections.get("objections", [])[:6]
    objections_str = "\n".join([f'- "{o["verbatim"]}"' for o in obj_list])

    vendeur_block = ""
    if vendeur.get("offre"):
        v = vendeur
        vendeur_block = f"""
CE QUE LE VENDEUR VA PROPOSER :
Il repr√©sente {v['entreprise']['nom']} et vend {v['offre']['nom']} : {v['offre']['description']}.
Prix : {v['offre'].get('prix', 'non communiqu√©')}.
"""

    return PROMPT_CONSTRAINTS + f"""SC√âNARIO MULTI-INTERLOCUTEURS ‚Äî Tu joues 2 r√¥les simultan√©ment.

INTERLOCUTEUR 1 : {name1}, {id1['poste']} chez {id1['entreprise']['nom']} ({id1['entreprise'].get('secteur', '')})
PERSONNALIT√â : {traits1}
TICS : {tics1}
FOCUS : {p1['psychologie'].get('motivations_profondes', ['efficacit√©'])[0] if p1['psychologie'].get('motivations_profondes') else 'efficacit√©'}

INTERLOCUTEUR 2 : {name2}, {id2['poste']} chez {id2['entreprise']['nom']}
PERSONNALIT√â : {traits2}
TICS : {tics2}
FOCUS : {p2['psychologie'].get('motivations_profondes', ['rigueur'])[0] if p2['psychologie'].get('motivations_profondes') else 'rigueur'}

CONTEXTE :
- Situation : {ctx1['situation_entreprise']}
- Priorit√©s : {', '.join(ctx1.get('priorites_actuelles', ['croissance'])[:2])}

DYNAMIQUE ENTRE LES DEUX :
{dynamique}

{vendeur_block}

DIFFICULT√â MULTI-INTERLOCUTEURS (toujours avanc√©) :
Convaincre un comit√© est toujours difficile. Chaque interlocuteur a ses propres crit√®res. Une r√©ponse qui satisfait l'un peut inqui√©ter l'autre.
- Chacun est m√©fiant sur SES sujets (budget, technique, op√©rationnel)
- Les objections se croisent et s'encha√Ænent entre les deux
- Le commercial doit convaincre les DEUX ‚Äî pas seulement le d√©cideur
- Si un argument est trop faible, l'autre interlocuteur surench√©rit
- Tipping point : les DEUX doivent voir un b√©n√©fice concret pour leur p√©rim√®tre

R√àGLES DE JEU MULTI-INTERLOCUTEURS :
1. Tu PR√âFIXES chaque r√©plique par le nom entre crochets : [{name1}] ou [{name2}]
2. Un seul interlocuteur parle par r√©plique. Tu alternes naturellement.
3. {name1} parle en premier (c'est lui/elle qui a organis√© le rendez-vous).
4. Quand le vendeur r√©pond √† l'un, l'autre peut r√©agir ENSUITE (pas les deux en m√™me temps).
5. Ils peuvent se contredire entre eux, ou l'un peut appuyer l'autre.
6. Si le vendeur ne s'adresse qu'√† un seul, l'autre finit par intervenir : "Excusez-moi, mais..."
7. La d√©cision finale requiert l'accord des DEUX.
8. Chaque interlocuteur a son propre niveau d'int√©r√™t (commence √† 2/10 chacun).

CONVERSATION NATURELLE :
- Phrases courtes, oral fran√ßais naturel. 1-3 phrases max par r√©plique.
- Tu ne fais JAMAIS de bruits type "hm", "hmm", "mmh"
- Tu vouvoies TOUJOURS. Tu ne sors JAMAIS du personnage.
- IMPORTANT : Pas d'abr√©viations orales. "rendez-vous" et non "rdv", "mille euros" et non "k‚Ç¨".

OBJECTIONS DISPONIBLES (r√©parties entre les deux) :
{objections_str}

COMPL√âMENTS ANTI-INVENTION :
Vous d√©couvrez le commercial pour la premi√®re fois. Aucun document re√ßu avant ce rendez-vous.

FIN :
- Les DEUX int√©ress√©s + next step ‚Üí accord
- Un seul convaincu ‚Üí "Il faut qu'on en reparle en interne"
- Aucun convaincu apr√®s 5 min ‚Üí fin polie
"""


def build_system_prompt(scenario: dict, difficulty: int = 2, language: str = "fr") -> str:
    persona = scenario["persona"]
    objections = scenario["objections"]
    vendeur = scenario.get("vendeur", {})

    # --- Multi-interlocuteurs : prompt sp√©cifique ---
    persona2 = scenario.get("persona_2")
    if persona2:
        return _build_multi_prompt(scenario, difficulty, language)

    # Identit√© prospect
    p = persona["identite"]
    prenom, nom, poste = p["prenom"], p["nom"], p["poste"]
    entreprise = p["entreprise"]["nom"]
    secteur = p["entreprise"].get("secteur", "")

    # Psychologie
    traits = ", ".join(persona["psychologie"]["traits_dominants"])
    style = persona["psychologie"]["style_communication"]

    # Contexte
    ctx = persona["contexte_actuel"]
    situation = ctx["situation_entreprise"]
    priorites = ", ".join(ctx.get("priorites_actuelles", [])[:2]) if ctx.get("priorites_actuelles") else "croissance, rentabilit√©"

    # Motivations et peurs
    motivations = ", ".join(persona["psychologie"].get("motivations_profondes", [])[:2]) if persona["psychologie"].get("motivations_profondes") else "optimiser ses r√©sultats"
    peurs = ", ".join(persona["psychologie"].get("peurs_freins", [])[:2]) if persona["psychologie"].get("peurs_freins") else "perdre du temps, se faire avoir"

    # Tics
    tics = persona["comportement_en_rdv"].get("tics_langage", [])
    tics_str = ", ".join(tics[:4]) if tics else '"Bon...", "√âcoutez...", "Concr√®tement ?"'

    # Objections (max 5)
    obj_list = objections.get("objections", [])[:5]
    objections_str = "\n".join([f'- "{o["verbatim"]}"' for o in obj_list])

    # Contexte vendeur (si disponible)
    vendeur_block = ""
    if vendeur.get("offre"):
        v = vendeur
        vendeur_block = f"""
CE QUE LE VENDEUR VA TE PROPOSER :
Il repr√©sente {v['entreprise']['nom']} et vend {v['offre']['nom']} : {v['offre']['description']}.
Prix : {v['offre'].get('prix', 'non communiqu√©')}.
Son objectif probable : {v['objectif_appel']['description']}.
Ses r√©f√©rences : {', '.join(v['offre'].get('references', []))}.

TU R√âAGIS √Ä CETTE OFFRE SP√âCIFIQUE :
- Si les r√©f√©rences sont dans ton secteur, √ßa t'int√©resse un peu plus (mais tu ne bloques pas dessus)
- Si la proposition r√©sout un probl√®me que tu VIS, tu ne peux pas l'ignorer
- Si c'est g√©n√©rique et pas adapt√© √† ton m√©tier, tu coupes court
- Tu ne poses PAS plus d'une question sur les r√©f√©rences ‚Äî apr√®s, tu passes √† autre chose (prix, d√©lai, mise en place)
"""

    # Blocs difficult√© ‚Äî instructions comportementales d√©taill√©es
    DIFF = {
        1: """NIVEAU D√âBUTANT ‚Äî Prospect accessible :
- Tu es de bonne humeur, disponible, poli
- Tu laisses le commercial finir ses phrases
- Tes objections sont simples et classiques : prix, d√©lai, besoin de r√©fl√©chir
- Tu acceptes un RDV apr√®s 2-3 √©changes convaincants
- Tu ne coupes pas la parole
- Tu donnes des indices clairs sur ta douleur
- Si le commercial fait une bonne accroche, tu deviens rapidement r√©ceptif
- Tu peux √™tre convaincu en 3-4 minutes""",
        2: """NIVEAU INTERM√âDIAIRE ‚Äî Prospect neutre :
- Tu es occup√© mais pas hostile
- Tu laisses le commercial parler mais tu poses des questions pr√©cises
- 2-3 objections r√©elles que tu maintiens jusqu'√† preuve convaincante
- Tu demandes des r√©f√©rences ou des chiffres avant de t'engager
- Tu peux raccrocher si l'accroche est nulle
- Ta douleur cach√©e n'√©merge qu'apr√®s au moins 3 bonnes questions de d√©couverte
- Tu peux √™tre convaincu en 6-8 minutes avec les bons arguments""",
        3: """NIVEAU AVANC√â ‚Äî Prospect difficile :
- Tu es m√©fiant, press√©, sceptique
- Tu coupes la parole si c'est trop long
- Tes objections sont dures et encha√Æn√©es : tu ne l√¢ches pas facilement
- Tu demandes des cas chiffr√©s PR√âCIS dans TON secteur ‚Äî tu refuses le g√©n√©rique
- Tu peux raccrocher sans pr√©venir apr√®s 2 minutes si pas d'accroche
- Ta douleur cach√©e est profond√©ment enfouie, elle n'√©merge QUE si le commercial creuse avec au moins 4-5 bonnes questions
- Tu as un pr√©jug√© n√©gatif sur les commerciaux ('encore un qui veut me vendre quelque chose')
- Tipping point difficile : n√©cessite une preuve concr√®te + un engagement de r√©sultat
- Tu peux √™tre convaincu mais √ßa prend 10-15 minutes de travail s√©rieux"""
    }

    # Language block for non-French simulations
    lang_block = ""
    if language != "fr":
        li = LANGUAGE_INFO.get(language, LANGUAGE_INFO["en"])
        lang_block = f"""
LANGUE DE SIMULATION : {li['name']}
Tu es un prospect {li['nationality']} et tu parles UNIQUEMENT en {li['name']}.
Tu ne comprends pas le fran√ßais. Si le commercial te parle en fran√ßais,
tu r√©ponds poliment en {li['name']} que tu ne parles pas fran√ßais.
Adapte tes expressions, ton style et tes r√©f√©rences culturelles au march√© {li['country']}.
"""

    prompt = PROMPT_CONSTRAINTS + f"""Tu es {prenom} {nom}, {poste} chez {entreprise} ({secteur}).

PERSONNALIT√â : {traits} | Style : {style}
TICS DE LANGAGE : {tics_str}

CONTEXTE R√âEL :
- Situation : {situation}
- Priorit√©s : {priorites}

TES MOTIVATIONS CACH√âES : {motivations}
TES PEURS : {peurs}

{vendeur_block}

{DIFF.get(difficulty, DIFF[2])}
{lang_block}
COMMENT TU FONCTIONNES :

1. TON √âTAT INTERNE : Ton int√©r√™t commence √† 2/10.
   - Bonne question sur tes VRAIS probl√®mes : +1 √† +2
   - Argument chiffr√© pertinent pour TON secteur : +1 √† +2
   - Pitch g√©n√©rique ou monologue : -1 √† -2
   - R√©f√©rence v√©rifiable dans ton secteur : +2
   - Ton comportement REFL√àTE ce niveau : √† 2 tu es ferm√©, √† 5 tu √©coutes, √† 7+ tu es int√©ress√©

2. RATIONALIT√â : Tu es un {poste}. Tu fais des calculs.
   - Si le vendeur chiffre un gain cr√©dible > ton co√ªt per√ßu, ton int√©r√™t monte
   - Si le vendeur mentionne un probl√®me que tu VIS R√âELLEMENT, tu ne peux pas l'ignorer
   - Si le vendeur dit quelque chose de faux sur ton secteur, tu corriges et ton int√©r√™t baisse

3. M√âMOIRE : Tu te souviens de TOUT dans cet appel.
   - Ne redemande pas une info d√©j√† donn√©e
   - Si le vendeur se contredit, rel√®ve-le
   - Si le vendeur revient sur un point valid√©, ne le rebloque pas

4. CONVERSATION NATURELLE :
   - Phrases courtes, oral fran√ßais naturel. "ouais", "bon", "√©coutez", "d'accord"
   - Tu ne fais JAMAIS de bruits type "hm", "hmm", "mmh"
   - 1-3 phrases max par r√©ponse
   - Tu peux couper si le vendeur monologue >15s
   - Tu vouvoies TOUJOURS, m√™me si le vendeur te tutoie
   - Tu ne sors JAMAIS du personnage
   - IMPORTANT : Tu ne dois JAMAIS utiliser d'abr√©viations dans tes r√©ponses orales. √âcris toujours les mots en entier : "rendez-vous" et non "rdv", "mille euros" et non "k‚Ç¨", "quatorze heures" et non "14h", "chiffre d'affaires" et non "CA". Tu parles, tu n'√©cris pas un SMS.

OBJECTIONS DISPONIBLES (utilise quand c'est PERTINENT, pas dans l'ordre) :
{objections_str}

COMPL√âMENTS ANTI-INVENTION :
Tu d√©couvres le commercial pour la premi√®re fois. Tu n'as re√ßu aucun document avant cet appel.
Si le commercial mentionne un document envoy√© : "Je ne me souviens pas d'avoir re√ßu quelque chose." ‚Äî jamais tu n'inventes un contenu.
Tu ne confirmes JAMAIS avoir vu, lu ou re√ßu quoi que ce soit que ton brief ne mentionne pas.
Si le commercial cite un tarif : tu r√©agis UNIQUEMENT √† CE tarif. Tu n'en inventes jamais un autre.

D√âBUT : Tu as d√©j√† d√©croch√© le t√©l√©phone. Le vendeur va parler. Tu attends sa premi√®re phrase pour r√©pondre.

FIN :
- Int√©r√™t >= 7 et next step propos√© : accepte naturellement
- Int√©r√™t 4-6 et bonne proposition : "envoyez-moi un r√©sum√© par mail"
- Int√©r√™t < 4 apr√®s 3 min : mets fin poliment
"""
    return prompt


# --- Preload VAD + scenarios cache ---
def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()
    _ensure_scenarios_db()  # Pre-populate scenario cache in worker process


# --- Entrypoint ---
server = AgentServer()


@server.rtc_session()
async def entrypoint(ctx: JobContext):
    t_entry = time.time()
    logger.info("üéØ Nouvelle session VendMieux")

    await ctx.connect()
    t_connected = time.time()
    logger.info(f"‚è±Ô∏è [latency] connect: {(t_connected - t_entry)*1000:.0f}ms")

    # Wait for the user participant to get their metadata
    participant = await ctx.wait_for_participant()
    t_participant = time.time()
    logger.info(f"‚è±Ô∏è [latency] participant joined: {(t_participant - t_entry)*1000:.0f}ms")

    # Load scenario and settings from participant metadata (set via access token)
    scenario_id = None
    difficulty = 2
    raw_meta = participant.metadata or ctx.room.metadata or ""
    logger.info(f"üì¶ Raw metadata: {raw_meta[:200] if raw_meta else '(empty)'}")

    meta_user_id = None
    meta_session_db_id = None
    meta_language = "fr"

    if raw_meta:
        try:
            meta = json.loads(raw_meta)
            scenario_id = meta.get("scenario_id")
            difficulty = int(meta.get("difficulty", 2))
            meta_user_id = meta.get("user_id")
            meta_session_db_id = meta.get("session_db_id")
            meta_language = meta.get("language", "fr")
        except (json.JSONDecodeError, ValueError):
            # Legacy: metadata is just a scenario_id string
            scenario_id = raw_meta if raw_meta else None

    scenario = None
    if scenario_id:
        scenario = load_scenario(scenario_id)
        logger.info(f"üìã Sc√©nario charg√© : {scenario_id}")

    if not scenario:
        scenario = DEFAULT_SCENARIO
        logger.info("üìã Sc√©nario par d√©faut (Olivier Bertrand, industrie)")

    t_scenario = time.time()
    logger.info(f"‚è±Ô∏è [latency] scenario loaded: {(t_scenario - t_entry)*1000:.0f}ms")

    # Fallback difficult√© depuis le sc√©nario si pas dans la metadata
    if difficulty == 2 and not raw_meta:
        difficulty = scenario.get("simulation", {}).get("difficulte", 2)

    # Construire le prompt avec intelligence situationnelle
    system_prompt = build_system_prompt(scenario, difficulty, language=meta_language)
    logger.info(f"üìù System prompt (diff={difficulty}, lang={meta_language}) : {len(system_prompt)} caract√®res")

    # Pr√©parer le greeting pr√©-script√© (bypass LLM)
    greeting = get_greeting(meta_language, scenario["persona"], scenario.get("persona_2"))
    logger.info(f"üëã Greeting pr√©-script√© : \"{greeting}\"")

    # Cr√©er l'agent prospect
    prospect = VendMieuxProspect(
        system_prompt=system_prompt,
        scenario_name=scenario["persona"]["identite"]["prenom"]
        + " "
        + scenario["persona"]["identite"]["nom"],
    )

    t_agent = time.time()
    logger.info(f"‚è±Ô∏è [latency] agent created: {(t_agent - t_entry)*1000:.0f}ms")

    # --- Transcript capture c√¥t√© serveur ---
    transcript_entries = []
    session_start = time.time()

    def on_conversation_item(ev):
        """Capture chaque message (user ou assistant) dans le transcript."""
        item = ev.item
        text = item.text_content
        if not text or not text.strip():
            return
        role = "vendeur" if item.role == "user" else "prospect"
        transcript_entries.append({
            "role": role,
            "text": text.strip(),
            "timestamp": time.time(),
        })
        logger.info(f"üìù [{role}] {text.strip()[:80]}")
        _save_room_history(ctx.room.name, session)

    _evaluation_sent = False

    async def _do_close_evaluation():
        """Coroutine d'√©valuation post-session."""
        nonlocal _evaluation_sent
        if _evaluation_sent:
            logger.info("‚è≠Ô∏è √âvaluation d√©j√† envoy√©e, skip doublon")
            return
        _evaluation_sent = True

        duration_s = int(time.time() - session_start)
        nb = len(transcript_entries)
        logger.info(f"üìä Session termin√©e ‚Äî {nb} entr√©es, {duration_s}s")

        if nb < 3:
            logger.info("‚è≠Ô∏è Trop peu d'√©changes pour √©valuer, skip")
            return

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                payload = {
                    "scenario_id": scenario_id or "__default__",
                    "difficulty": difficulty,
                    "duration_s": duration_s,
                    "transcript": transcript_entries,
                    "user_id": meta_user_id,
                    "session_db_id": meta_session_db_id,
                    "language": meta_language,
                }
                resp = await client.post(
                    "http://127.0.0.1:8000/api/evaluate",
                    json=payload,
                )
                if resp.status_code == 200:
                    score = resp.json().get("score_global", "?")
                    logger.info(f"‚úÖ √âvaluation re√ßue ‚Äî Score: {score}/20")
                else:
                    logger.warning(f"‚ö†Ô∏è √âvaluation √©chou√©e ({resp.status_code}): {resp.text[:200]}")
        except Exception as e:
            logger.error(f"‚ùå Erreur envoi √©valuation: {e}")

    def on_close(ev):
        """Sync callback ‚Äî sauvegarde historique puis lance l'√©valuation."""
        _save_room_history(ctx.room.name, session)
        asyncio.create_task(_do_close_evaluation())

    # Multi-language voice/STT configuration
    # Voix par d√©faut (masculines) ‚Äî utilis√©es comme fallback pour les langues non-FR
    VOICE_MAP = {
        "fr": {"language_code": "fr-FR"},
        "en": {"voice": "en-GB-Chirp3-HD-Charon", "language_code": "en-GB"},
        "es": {"voice": "es-ES-Chirp3-HD-Charon", "language_code": "es-ES"},
        "de": {"voice": "de-DE-Chirp3-HD-Charon", "language_code": "de-DE"},
        "it": {"voice": "it-IT-Chirp3-HD-Charon", "language_code": "it-IT"},
    }
    STT_LANG_MAP = {
        "fr": "fr", "en": "en", "es": "es", "de": "de", "it": "it",
    }
    voice_cfg = VOICE_MAP.get(meta_language, VOICE_MAP["fr"])
    stt_lang = STT_LANG_MAP.get(meta_language, "fr")

    # D√©tection genre via Claude pour s√©lection voix TTS
    persona_desc = system_prompt[:200]  # D√©but du prompt contient l'identit√©
    sid = scenario_id or "__default__"
    gender = get_cached_gender(sid, persona_desc)

    if meta_language == "fr":
        # FR : voix Chirp3-HD genr√©es (haute qualit√©)
        tts_voice = VOIX_FEMININES[0] if gender == 'F' else VOIX_MASCULINES[0]
        voice_cfg["voice"] = tts_voice
    logger.info(f"üéôÔ∏è Voix TTS s√©lectionn√©e : {voice_cfg['voice']} (genre={gender})")

    # Cr√©er la session avec le pipeline STT ‚Üí LLM ‚Üí TTS
    session = AgentSession(
        vad=silero.VAD.load(),
        stt=deepgram.STT(
            model="nova-3",
            language=stt_lang,
        ),
        llm=anthropic.LLM(
            model="claude-haiku-4-5-20251001",
            temperature=0.2,
            max_tokens=150,
        ),
        tts=google_tts.TTS(
            voice_name=voice_cfg["voice"],
            language=voice_cfg["language_code"],
            speaking_rate=1.0,
        ),
    )

    # Brancher les callbacks de capture
    session.on("conversation_item_added", on_conversation_item)
    session.on("close", on_close)

    await session.start(
        room=ctx.room,
        agent=prospect,
    )

    t_session_started = time.time()
    logger.info(f"‚è±Ô∏è [latency] session started: {(t_session_started - t_entry)*1000:.0f}ms")

    # V√©rifier si c'est une reconnexion (historique existant pour cette room)
    room_name = ctx.room.name
    prev_history = _pop_room_history(room_name)

    if prev_history is not None:
        from livekit.agents.llm import ChatContext
        restored = ChatContext.from_dict(prev_history)
        session.history.merge(restored)
        nb_restored = len(restored.items)
        logger.info(f"üîÑ Reconnexion ‚Äî {nb_restored} items restaur√©s pour room {room_name}")
        session.say("Excusez-moi, on en √©tait o√π ?", allow_interruptions=True)
    else:
        # Le prospect d√©croche ‚Äî greeting pr√©-script√© (bypass LLM ‚Üí TTS direct, gain ~2-3s)
        session.say(greeting, allow_interruptions=False)

    t_greeting_sent = time.time()
    logger.info(f"‚è±Ô∏è [latency] greeting sent to TTS: {(t_greeting_sent - t_entry)*1000:.0f}ms")
    logger.info(f"‚è±Ô∏è [latency] TOTAL entrypoint‚Üígreeting: {(t_greeting_sent - t_entry)*1000:.0f}ms")


if __name__ == "__main__":
    agents.cli.run_app(server)
